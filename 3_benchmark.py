import pandas as pd
import argparse
import json
import parameters
import os.path
import time
from codecarbon import OfflineEmissionsTracker
from river import metrics
from river import stream
from river import multioutput
from river import tree
from river import forest
from river import preprocessing
from river import dummy
from river import stats
from sklearn.multioutput import MultiOutputClassifier
from sklearn.model_selection import ParameterGrid
import bench_metrics.RMSE
import bench_metrics.precisionatk
import implemented_models.NN
import implemented_models.NN_TL
import implemented_models.NN_TLH
import implemented_models.NN_TLH_fifo
import implemented_models.NN_TLH_sampling
import implemented_models.NN_TLH_memories
import implemented_models.NN_TLH_mini_memories
import implemented_models.NN_TLH_attention
import implemented_models.NN_HybridAdaptive
import implemented_models.Ensemble_MLC
import implemented_models.binary_relevance
import implemented_models.baseline_1_NN
import random


### Setting variables ########################################################

parser = argparse.ArgumentParser(description="Main")
parser.add_argument(
    "--scenario",
    type=str,
    default="task_based",
    help="Other choice : task_free. Default to task_based.",
)
parser.add_argument(
    "--ordering",
    type=str,
    default="grad",
    help="grad for gradual drift, sudden for sudden drift, random for random ordering",
)
parser.add_argument(
    "--seed",
    type=int,
    default=1,
    help="seed, default to 1",
)
args = parser.parse_args()

scenario = args.scenario
ordering = args.ordering
seed = args.seed

bench = parameters.benchmark()
datasets = bench.datasets
models = bench.models
workdir = bench.workdir

### Functions ###################################################################


def get_ordering(k_data, workdir: str, scenario: str):
    """Get the ordering of the clusters in the stream according to the used ordering.

    Args:
        ordering (str): ordering used

    Returns:
        cluster_order (dict) : a dict containing the order of the clusters
    """
    assert os.path.isfile(
        workdir + "Orders/{}_random_".format(k_data) + "{}.json".format(scenario)
    ), "ordering not found in datasets directory"
    with open(
        workdir + "Orders/{}_random_".format(k_data) + "{}.json".format(scenario),
        "r",
        encoding="utf-8",
    ) as json_file:
        cluster_order = json.load(json_file)
    return cluster_order


def get_label_signature(k_data: str, workdir: str):
    """Get the label signature of each cluster.

    Args:
        k_data (str): name of the dataset

    Returns:
        cluster_label_signature (dict): dict containing the label signature of each label
    """
    assert os.path.isfile(
        workdir + "Labels/{}_cluster_labels_task_based.json".format(k_data)
    ), "label signature file not found in datasets directory"
    with open(
        workdir + "Labels/{}_cluster_labels_task_based.json".format(k_data),
        "r",
        encoding="utf-8",
    ) as json_file:
        cluster_label_signature = json.load(json_file)
    return cluster_label_signature


def get_task_based_data(
    cluster_order: dict,
    full_stream: dict,
    eval_sets: dict,
    k_data: str,
    workdir: str,
    scenario: str,
    ordering: str,
):
    """Get the data for a task based scenario.

    Args:
        cluster_order (dict): the dict containing the order of the clusters
        full_stream (dict): dict that will be filled with the full ordered data stream
        eval_sets (dict): the dict containing the eval sets for each cluster
        k_data (str): name of the dataset
    """
    seen_cluster = []
    stream_segmentation = dict()
    assert os.path.isfile(
        workdir + "Length/{}_".format(k_data) + "stream_length_{}.json".format(scenario)
    ), "length file not found"
    with open(
        workdir
        + "Length/{}_".format(k_data)
        + "stream_length_{}.json".format(scenario),
        "r",
        encoding="utf-8",
    ) as json_file:
        cluster_length = json.load(json_file)
    for k_order, v_order in cluster_order.items():
        if v_order not in seen_cluster:
            full_stream[k_order] = [
                v_order,
                pd.read_csv(
                    workdir
                    + "Experiences/{}".format(k_data)
                    + "_{}_".format(v_order)
                    + "exp_1.csv"
                ),
            ]
            data_eval = pd.read_csv(
                workdir
                + "Eval_set/{}".format(k_data)
                + "_{}_".format(v_order)
                + "eval_set.csv"
            )
            X_eval = data_eval.iloc[:, : (-1) * (v_data[0])]
            Y_eval = data_eval.iloc[:, (-1) * (v_data[0]) :]
            eval_sets[v_order] = [X_eval, Y_eval]
            seen_cluster.append(v_order)
            stream_segmentation[k_order] = [
                "cluster_{}_exp_1".format(v_order),
                cluster_length["cluster_{}_exp_1".format(v_order)],
            ]
        elif v_order in seen_cluster:
            full_stream[k_order] = [
                v_order,
                pd.read_csv(
                    workdir
                    + "Experiences/{}".format(k_data)
                    + "_{}_".format(v_order)
                    + "exp_2.csv"
                ),
            ]
            stream_segmentation[k_order] = [
                "cluster_{}_exp_2".format(v_order),
                cluster_length["cluster_{}_exp_2".format(v_order)],
            ]
    output_file = (
        workdir
        + "Length/{}_".format(k_data)
        + "stream_segmentation_{}_".format(scenario)
        + "{}.json".format(ordering)
    )
    with open(output_file, "w") as outfile:
        json.dump(stream_segmentation, outfile)


def online_eval(
    online_metrics_dict: dict,
    y: dict,
    y_pred: dict,
    metrics_online_results: dict,
    task: int,
):
    """Update the online metrics for the evaluation.

    Args:
        online_metrics_dict (dict): dict containing the online metrics.
        y (dict): dict containing the true label vector.
        y_pred (dict): predicted label vector
        metrics_online_results (dict): dict containing the online metrics results
    """
    for k in online_metrics_dict.keys():
        #TODO remove
        # try:
        #     result = online_metrics_dict[k].get()
        #     if result is None:
        #         print(f"{k}: No data available yet")
        #     else:
        #         print(f"{k} : {result}")
        # except Exception as e:
        #     print(f"Warning: Could not retrieve metric for {k}. Error: {e}")
            
        if k == "RMSE" or k == "precisionatk":
            for k_pred in y.keys():
                if k_pred not in y_pred:
                    y_pred[k_pred] = 0
            for k_pred in y_pred.keys():
                if k_pred not in y:
                    y[k_pred] = 0
                elif y_pred[k_pred] == None:
                    y_pred[k_pred] = 0
            masked_y = dict()
            masked_y_pred = dict()
            for k_y in y.keys():
                if k_y in cluster_label_signature["Cluster {}".format(task)]:
                    masked_y[k_y] = y[k_y]
                    masked_y_pred[k_y] = y_pred[k_y]
            #TODO remove
            # print(f"Processing metric: {k}")
            # print(f"masked_y: {masked_y}, masked_y_pred: {masked_y_pred}")

            # Check if update() is being called
            if masked_y and masked_y_pred:
                online_metrics_dict[k].update(masked_y, masked_y_pred)
            else:
                print(f"Skipping update for {k} - Empty masked_y or masked_y_pred")


            online_metrics_dict[k].update(masked_y, masked_y_pred)
            # print("{}".format(k) + " : " + "{}".format(online_metrics_dict[k].get()))
            metrics_online_results[k].append(online_metrics_dict[k].get())
        else:
            for k_pred in y_pred.keys():
                if y_pred[k_pred] == None or float(y_pred[k_pred]) < 0.5:
                    y_pred[k_pred] = 0
                elif float(y_pred[k_pred]) > 0.5:
                    y_pred[k_pred] = 1
            for k_pred in y.keys():
                if k_pred not in y_pred:
                    y_pred[k_pred] = 0
            for k_pred in y_pred.keys():
                if k_pred not in y:
                    y[k_pred] = 0
            masked_y = dict()
            masked_y_pred = dict()
            for k_y in y.keys():
                if k_y in cluster_label_signature["Cluster {}".format(task)]:
                    masked_y[k_y] = y[k_y]
                    masked_y_pred[k_y] = y_pred[k_y]

            #TODO remove
            # print(f"Processing metric: {k}")
            # print(f"masked_y: {masked_y}, masked_y_pred: {masked_y_pred}")

            # Check if update() is being called
            if masked_y and masked_y_pred:
                online_metrics_dict[k].update(masked_y, masked_y_pred)
            else:
                print(f"Skipping update for {k} - Empty masked_y or masked_y_pred")
            online_metrics_dict[k].update(masked_y, masked_y_pred)
            # print("{}".format(k) + " : " + "{}".format(online_metrics_dict[k].get()))
            metrics_online_results[k].append(online_metrics_dict[k].get())


def first_exp_HPO(model_test, full_stream, v_data, cluster_label_signature):
    hpo_accuracy = metrics.multioutput.MacroAverage(metrics.BalancedAccuracy())
    if not full_stream:
        raise ValueError("full_stream is empty; no data to process")
    first_key = next(iter(full_stream)) 
    X_frame = full_stream[first_key][1].iloc[:, : (-1) * (v_data[0])]
    Y_frame = full_stream[first_key][1].iloc[:, (-1) * (v_data[0]) :]

    # Initializing the code carbon tracker
    tracker = OfflineEmissionsTracker(
        tracking_mode="process",
        country_iso_code="FRA",
    )
    tracker.start()

    for x, y in stream.iter_pandas(X_frame, Y_frame):
        y_pred = model_test.predict_one(x)
        # Online evaluation :
        for k_pred in y_pred.keys():
            if y_pred[k_pred] == None or float(y_pred[k_pred]) < 0.5:
                y_pred[k_pred] = 0
            elif float(y_pred[k_pred]) > 0.5:
                y_pred[k_pred] = 1
        for k_pred in y.keys():
            if k_pred not in y_pred:
                y_pred[k_pred] = 0
        for k_pred in y_pred.keys():
            if k_pred not in y:
                y[k_pred] = 0
        masked_y = dict()
        masked_y_pred = dict()
        for k_y in y.keys():
            if k_y in cluster_label_signature["Cluster {}".format(full_stream["0"][0])]:
                masked_y[k_y] = y[k_y]
                masked_y_pred[k_y] = y_pred[k_y]
        hpo_accuracy.update(masked_y, masked_y_pred)
        # Training
        if (
            m == "NN_TL"
            or m == "NN_TLH"
            or m == "NN_TLH_fifo"
            or m == "NN_TLH_sampling"
            or m == "NN_TLH_memories"
            or m == "NN_TLH_mini_memories"
            or m == "NN_TLH_attention"
            or m == "NN_HybridAdaptive"
        ):
            model_test.learn_one(
                x,
                y,
                cluster_label_signature["Cluster {}".format(full_stream["0"][0])],
            )
        else:
            model_test.learn_one(x, y)
    tracker.stop()
    frugality = hpo_accuracy.get() - (1 / (1 + (1 / tracker._total_cpu_energy.kWh)))
    # print("Accuracy : {}".format(hpo_accuracy.get()))
    # print("Frugalité : {}".format(tracker._total_cpu_energy.kWh))
    return frugality


def init_continual_metrics(
    eval_sets: dict,
    continual_metrics_dict: dict,
    metrics_continual_results: dict,
    iter_continual: int,
):
    """Initializes the continual metrics for each cluster on this round of continual evaluation.

    Args:
        eval_sets (dict): dict containing the evaluation sets data
        continual_metrics_dict (dict): dict containing the continual evaluation metrics
        metrics_continual_results (dict): dict that will be filled with metrics values.
        iter_continual (int): continual evaluation round number
    """
    for i in range(len(eval_sets)):
        continual_metrics_dict[
            "continual_macro_BA_cluster_{}_".format(i)
            + "round_{}".format(iter_continual)
        ] = metrics.multioutput.MacroAverage(metrics.BalancedAccuracy())
        metrics_continual_results[
            "continual_macro_BA_cluster_{}_".format(i)
            + "round_{}".format(iter_continual)
        ] = []


def continual_eval(
    continual_metrics_dict: dict,
    cl: int,
    iter_continual: int,
    y: dict,
    y_pred: dict,
):
    """Updates the metrics for continual evaluation.

    Args:
        continual_metrics_dict (dict): dict containing the continual evaluation metrics
        cl (int): number of cluster
        iter_continual (int): continual evaluation round number
        y (dict): dict containing the true label vector.
        y_pred (dict): predicted label vector
    """
    for k in y_pred.keys():
        if y_pred[k] == None or float(y_pred[k]) < 0.5:
            y_pred[k] = 0
        elif float(y_pred[k]) >= 0.5:
            y_pred[k] = 1
    for k in y.keys():
        if k not in y_pred:
            y_pred[k] = 0
    for k in y_pred.keys():
        if k not in y:
            y[k] = 0
    masked_y = dict()
    masked_y_pred = dict()
    for k_y in y.keys():
        if k_y in cluster_label_signature["Cluster {}".format(cl)]:
            masked_y[k_y] = y[k_y]
            masked_y_pred[k_y] = y_pred[k_y]
    continual_metrics_dict[
        "continual_macro_BA_cluster_{}_".format(cl) + "round_{}".format(iter_continual)
    ].update(masked_y, masked_y_pred)
    # print(
    #     "continual_macro_BA_cluster_{}".format(cl)
    #     + " : "
    #     + "{}".format(
    #         continual_metrics_dict[
    #             "continual_macro_BA_cluster_{}_".format(cl)
    #             + "round_{}".format(iter_continual)
    #         ].get()
    #     )
    # )


def continual_results_saving(
    metrics_continual_results: dict,
    continual_metrics_dict: dict,
    cl: int,
    iter_continual: int,
):
    """Save continual evaluation metrics results.

    Args:
        metrics_continual_results (dict): dict that is filled with metrics values.
        continual_metrics_dict (dict): dict containing the continual evaluation metrics
        cl (int): number of cluster
        iter_continual (int): continual evaluation round number
    """
    metrics_continual_results[
        "continual_macro_BA_cluster_{}_".format(cl) + "round_{}".format(iter_continual)
    ].append(
        continual_metrics_dict[
            "continual_macro_BA_cluster_{}_".format(cl)
            + "round_{}".format(iter_continual)
        ].get()
    )


def save_bench_results(
    k_data: str,
    m: str,
    ordering: str,
    metrics_online_results: dict,
    metrics_continual_results: dict,
    workdir: str,
):
    """Save the metrics results in json files.

    Args:
        k_data (str): name of dataset
        m (str): name of model
        ordering (str): ordering used
        metrics_online_results (dict): dict containing the online metrics results
        metrics_continual_results (dict): dict containing the continual metrics values.
    """
    output_file = (
        workdir
        + "results/{}_".format(k_data)
        + "{}_".format(m)
        + "{}_online_".format(scenario)
        + "{}_results".format(ordering)
        + ".json"
    )
    with open(output_file, "w") as outfile:
        json.dump(metrics_online_results, outfile)
    output_file = (
        workdir
        + "results/{}_".format(k_data)
        + "{}_".format(m)
        + "{}_continual_".format(scenario)
        + "{}_results".format(ordering)
        + ".json"
    )
    with open(output_file, "w") as outfile:
        json.dump(metrics_continual_results, outfile)


###########################################################################################
############### Main ######################################################################
###########################################################################################

for k_data, v_data in datasets.items():

    # Getting ordering :
    cluster_order = get_ordering(k_data, workdir, scenario)

    if not cluster_order:
        print(f"Warning: cluster_order is empty for {k_data}, {scenario}. Skipping.")
        continue  # Skip to the next dataset in the loop

    # Getting label signature of each cluster :
    cluster_label_signature = get_label_signature(k_data, workdir)

    # Getting full_stream and eval_sets
    full_stream = dict()
    eval_sets = dict()
    get_task_based_data(
        cluster_order, full_stream, eval_sets, k_data, workdir, scenario, ordering
    )
    labels = []
    for i in range(v_data[0]):
        labels.append("y{}".format(i))

    for m in models:
        # Initializing the model

        models_parameter = {
            "NN": {"learning_rate": [0.1, 0.01, 0.001]},
            "NN_TL": {"learning_rate": [0.1, 0.01, 0.001]},
            "NN_TLH": {
                "learning_rate": [0.1, 0.01, 0.001],
                "hidden_sizes": [200, 2000],
            },
            "NN_TLH_fifo": {
                "learning_rate": [0.1, 0.01, 0.001],
                "hidden_sizes": [200, 2000],
                "size_replay_fifo": [100, 1000],
                "replay_fifo": [5, 10],
            },
            "NN_TLH_sampling": {
                "learning_rate": [0.1, 0.01, 0.001],
                "hidden_sizes": [200, 2000],
                "size_replay_sampling": [100, 1000],
                "replay_sampling": [5, 10],
            },
            "NN_TLH_memories": {
                "learning_rate": [0.1, 0.01, 0.001],
                "hidden_sizes": [200, 2000],
                "size_replay_fifo": [100, 1000],
                "replay_fifo": [5, 10],
                "size_replay_sampling": [100, 1000],
                "replay_sampling": [5, 10],
            },
            "NN_TLH_mini_memories": {
                "learning_rate": [0.1, 0.01, 0.001],
                "hidden_sizes": [200, 2000],
                "size_replay_fifo": [100, 1000],
                "replay_fifo": [5, 10],
                "size_replay_sampling": [100, 1000],
                "replay_sampling": [5, 10],
            },
            "NN_TLH_attention": {
                "learning_rate": [0.1, 0.01, 0.001],
                "hidden_sizes": [200, 2000],
                "size_replay": [100, 1000],
                "replay_k": [5, 10],
            },
            "NN_HybridAdaptive": {
                "learning_rate": [0.01, 0.001],
                "hidden1": [200, 400],
                "hidden2": [200],
                "size_fifo": [500, 1000],
                "size_reservoir": [500, 1000],
                "replay_samples": [5, 10],
            },
            "BR_HT": {
                "grace_period": [100, 200],
                "delta": [1e-06, 1e-07],
                "tau": [0.05, 0.1],
            },
            "LC_HT": {
                "grace_period": [100, 200],
                "delta": [1e-06, 1e-07],
                "tau": [0.05, 0.1],
            },
            "CC_HT": {
                "grace_period": [100, 200],
                "delta": [1e-06, 1e-07],
                "tau": [0.05, 0.1],
            },
            "BR_random_forest": {
                "n_models": [5, 10, 15],
                "grace_period": [50, 100],
                "delta": [1e-06, 1e-07],
                "tau": [0.05, 0.1],
            },
            "iSOUPtree": {
                "grace_period": [100, 200],
                "delta": [1e-06, 1e-07],
                "tau": [0.05, 0.1],
            },
            "baseline_last_class": {},
            "baseline_prior_distribution": {},
            "baseline_mean": {},
            "baseline_oracle": {},
            "baseline_1_NN": {
                "equal_null_cos": [True, False],
                "eucli": [True, False],
                "max_window_size": [0, 10, 100, 1000],
            },
            "Ensemble_MLC": {},
        }

        best_config = [-float('inf'), 0, 0]
        list_config = ParameterGrid(models_parameter[m])
        nb_config = 10

        if m == "Ensemble_MLC":
            # Load best configurations for sub-models
            with open(workdir + "Config/{}_NN_TLH_attention_{}.json".format(k_data, scenario), "r") as f:
                config_nn_tlh_attention = json.load(f)
            with open(workdir + "Config/{}_NN_TLH_mini_memories_{}.json".format(k_data, scenario), "r") as f:
                config_nn_tlh_mini_memories = json.load(f)
            with open(workdir + "Config/{}_BR_random_forest_{}.json".format(k_data, scenario), "r") as f:
                config_br_random_forest = json.load(f)

            # Initialize sub-models with best configurations
            model_nn_tlh_attention = implemented_models.NN_TLH_attention.NN_TLH_attention(
                learning_rate=config_nn_tlh_attention["learning_rate"],
                feature_size=v_data[1],
                hidden_sizes=config_nn_tlh_attention["hidden_sizes"],
                size_replay=config_nn_tlh_attention["size_replay"],
                replay_k=config_nn_tlh_attention["replay_k"],
                label_size=v_data[0],
            )
            model_nn_tlh_mini_memories = implemented_models.NN_TLH_mini_memories.NN_TLH_mini_memories_classifier(
                learning_rate=config_nn_tlh_mini_memories["learning_rate"],
                feature_size=v_data[1],
                hidden_sizes=config_nn_tlh_mini_memories["hidden_sizes"],
                size_replay_fifo=config_nn_tlh_mini_memories["size_replay_fifo"],
                replay_fifo=config_nn_tlh_mini_memories["replay_fifo"],
                size_replay_sampling=config_nn_tlh_mini_memories["size_replay_sampling"],
                replay_sampling=config_nn_tlh_mini_memories["replay_sampling"],
                label_size=v_data[0],
            )
            model_br_random_forest = implemented_models.binary_relevance.binary_relevance(
                forest.ARFClassifier(
                    n_models=config_br_random_forest["n_models"],
                    grace_period=config_br_random_forest["grace_period"],
                    delta=config_br_random_forest["delta"],
                    tau=config_br_random_forest["tau"],
                )
            )

            # Create ensemble model
            ensemble_model = implemented_models.Ensemble_MLC.EnsembleMultiLabelClassifier(
                models=[
                    (model_nn_tlh_attention, True),      # Requires signature
                    (model_nn_tlh_mini_memories, True),  # Requires signature
                    (model_br_random_forest, False),     # Does not require signature
                ]
            )

            # Set best_config without HPO
            best_config = [0, {}, ensemble_model]

        if nb_config < len(list_config):
            g = 0
            random_hpo = random.sample(range(len(list_config)), nb_config)
            for config in list_config:
                if g in random_hpo:
                    if m == "NN":
                        model_test = implemented_models.NN.NN_classifier(
                            learning_rate=config["learning_rate"],
                            feature_size=v_data[1],
                            label_size=v_data[0],
                        )
                        candidate_accuracy = first_exp_HPO(
                            model_test, full_stream, v_data, cluster_label_signature
                        )
                        if best_config[0] < candidate_accuracy:
                            best_config = [
                                candidate_accuracy,
                                config,
                                implemented_models.NN.NN_classifier(
                                    learning_rate=config["learning_rate"],
                                    feature_size=v_data[1],
                                    label_size=v_data[0],
                                ),
                            ]
                    elif m == "NN_TL":
                        model_test = implemented_models.NN_TL.NN_TL_classifier(
                            learning_rate=config["learning_rate"],
                            feature_size=v_data[1],
                            label_size=v_data[0],
                        )
                        candidate_accuracy = first_exp_HPO(
                            model_test, full_stream, v_data, cluster_label_signature
                        )
                        if best_config[0] < candidate_accuracy:
                            best_config = [
                                candidate_accuracy,
                                config,
                                implemented_models.NN_TL.NN_TL_classifier(
                                    learning_rate=config["learning_rate"],
                                    feature_size=v_data[1],
                                    label_size=v_data[0],
                                ),
                            ]
                    elif m == "NN_TLH":
                        model_test = implemented_models.NN_TLH.NN_TLH_classifier(
                            learning_rate=config["learning_rate"],
                            feature_size=v_data[1],
                            hidden_sizes=config["hidden_sizes"],
                            label_size=v_data[0],
                        )
                        candidate_accuracy = first_exp_HPO(
                            model_test, full_stream, v_data, cluster_label_signature
                        )
                        if best_config[0] < candidate_accuracy:
                            best_config = [
                                candidate_accuracy,
                                config,
                                implemented_models.NN_TLH.NN_TLH_classifier(
                                    learning_rate=config["learning_rate"],
                                    feature_size=v_data[1],
                                    hidden_sizes=config["hidden_sizes"],
                                    label_size=v_data[0],
                                ),
                            ]
                    elif m == "NN_TLH_fifo":
                        model_test = (
                            implemented_models.NN_TLH_fifo.NN_TLH_fifo_classifier(
                                learning_rate=config["learning_rate"],
                                feature_size=v_data[1],
                                hidden_sizes=config["hidden_sizes"],
                                label_size=v_data[0],
                                size_replay_fifo=config["size_replay_fifo"],
                                replay_fifo=config["replay_fifo"],
                            )
                        )
                        candidate_accuracy = first_exp_HPO(
                            model_test, full_stream, v_data, cluster_label_signature
                        )
                        if best_config[0] < candidate_accuracy:
                            best_config = [
                                candidate_accuracy,
                                config,
                                implemented_models.NN_TLH_fifo.NN_TLH_fifo_classifier(
                                    learning_rate=config["learning_rate"],
                                    feature_size=v_data[1],
                                    hidden_sizes=config["hidden_sizes"],
                                    label_size=v_data[0],
                                    size_replay_fifo=config["size_replay_fifo"],
                                    replay_fifo=config["replay_fifo"],
                                ),
                            ]
                    elif m == "NN_TLH_sampling":
                        model_test = implemented_models.NN_TLH_sampling.NN_TLH_sampling_classifier(
                            learning_rate=config["learning_rate"],
                            feature_size=v_data[1],
                            hidden_sizes=config["hidden_sizes"],
                            label_size=v_data[0],
                            size_replay_sampling=config["size_replay_sampling"],
                            replay_sampling=config["replay_sampling"],
                        )
                        candidate_accuracy = first_exp_HPO(
                            model_test, full_stream, v_data, cluster_label_signature
                        )
                        if best_config[0] < candidate_accuracy:
                            best_config = [
                                candidate_accuracy,
                                config,
                                implemented_models.NN_TLH_sampling.NN_TLH_sampling_classifier(
                                    learning_rate=config["learning_rate"],
                                    feature_size=v_data[1],
                                    hidden_sizes=config["hidden_sizes"],
                                    label_size=v_data[0],
                                    size_replay_sampling=config["size_replay_sampling"],
                                    replay_sampling=config["replay_sampling"],
                                ),
                            ]
                    elif m == "NN_TLH_memories":
                        model_test = implemented_models.NN_TLH_memories.NN_TLH_memories_classifier(
                            learning_rate=config["learning_rate"],
                            feature_size=v_data[1],
                            hidden_sizes=config["hidden_sizes"],
                            label_size=v_data[0],
                            size_replay_fifo=config["size_replay_fifo"],
                            replay_fifo=config["replay_fifo"],
                            size_replay_sampling=config["size_replay_sampling"],
                            replay_sampling=config["replay_sampling"],
                        )
                        candidate_accuracy = first_exp_HPO(
                            model_test, full_stream, v_data, cluster_label_signature
                        )
                        if best_config[0] < candidate_accuracy:
                            best_config = [
                                candidate_accuracy,
                                config,
                                implemented_models.NN_TLH_memories.NN_TLH_memories_classifier(
                                    learning_rate=config["learning_rate"],
                                    feature_size=v_data[1],
                                    hidden_sizes=config["hidden_sizes"],
                                    label_size=v_data[0],
                                    size_replay_fifo=config["size_replay_fifo"],
                                    replay_fifo=config["replay_fifo"],
                                    size_replay_sampling=config["size_replay_sampling"],
                                    replay_sampling=config["replay_sampling"],
                                ),
                            ]
                    elif m == "NN_TLH_mini_memories":
                        model_test = implemented_models.NN_TLH_mini_memories.NN_TLH_mini_memories_classifier(
                            learning_rate=config["learning_rate"],
                            feature_size=v_data[1],
                            hidden_sizes=config["hidden_sizes"],
                            label_size=v_data[0],
                            size_replay_fifo=config["size_replay_fifo"],
                            replay_fifo=config["replay_fifo"],
                            size_replay_sampling=config["size_replay_sampling"],
                            replay_sampling=config["replay_sampling"],
                        )
                        candidate_accuracy = first_exp_HPO(
                            model_test, full_stream, v_data, cluster_label_signature
                        )
                        if best_config[0] < candidate_accuracy:
                            best_config = [
                                candidate_accuracy,
                                config,
                                implemented_models.NN_TLH_mini_memories.NN_TLH_mini_memories_classifier(
                                    learning_rate=config["learning_rate"],
                                    feature_size=v_data[1],
                                    hidden_sizes=config["hidden_sizes"],
                                    label_size=v_data[0],
                                    size_replay_fifo=config["size_replay_fifo"],
                                    replay_fifo=config["replay_fifo"],
                                    size_replay_sampling=config["size_replay_sampling"],
                                    replay_sampling=config["replay_sampling"],
                                ),
                            ]
                    if m == "NN_TLH_attention":
                        # e.g. for each combination of hyperparams:
                        model_test = implemented_models.NN_TLH_attention.NN_TLH_attention_classifier(
                            learning_rate=config["learning_rate"],
                            feature_size=v_data[1],
                            hidden_sizes=config["hidden_sizes"],
                            label_size=v_data[0],
                            size_replay=config["size_replay"],
                            replay_k=config["replay_k"],
                        )

                        candidate_accuracy = first_exp_HPO(
                            model_test, full_stream, v_data, cluster_label_signature
                        )

                        if best_config[0] < candidate_accuracy:
                            best_config = [
                                candidate_accuracy,
                                config,
                                implemented_models.NN_TLH_attention.NN_TLH_attention_classifier(
                                    learning_rate=config["learning_rate"],
                                    feature_size=v_data[1],
                                    hidden_sizes=config["hidden_sizes"],
                                    label_size=v_data[0],
                                    size_replay=config["size_replay"],
                                    replay_k=config["replay_k"],
                                ),
                            ]
                    elif m == "NN_HybridAdaptive":
                        model_test = implemented_models.NN_HybridAdaptive.NN_HybridAdaptive_classifier(
                            learning_rate=config["learning_rate"],
                            feature_size=v_data[1],
                            label_size=v_data[0],
                            hidden1=config["hidden1"],
                            hidden2=config["hidden2"],
                            size_fifo=config["size_fifo"],
                            size_reservoir=config["size_reservoir"],
                            replay_samples=config["replay_samples"],
                        )

                        candidate_accuracy = first_exp_HPO(model_test, full_stream, v_data, cluster_label_signature)

                        if best_config[0] < candidate_accuracy:
                            best_config = [candidate_accuracy, config, model_test]

                    elif m == "BR_HT":
                        model_test = (
                            implemented_models.binary_relevance.binary_relevance(
                                model=tree.HoeffdingTreeClassifier(
                                    grace_period=config["grace_period"],
                                    delta=config["delta"],
                                    tau=config["tau"],
                                )
                            )
                        )
                        candidate_accuracy = first_exp_HPO(
                            model_test, full_stream, v_data, cluster_label_signature
                        )
                        if best_config[0] < candidate_accuracy:
                            best_config = [
                                candidate_accuracy,
                                config,
                                implemented_models.binary_relevance.binary_relevance(
                                    model=tree.HoeffdingTreeClassifier(
                                        grace_period=config["grace_period"],
                                        delta=config["delta"],
                                        tau=config["tau"],
                                    )
                                ),
                            ]
                    elif m == "LC_HT":
                        model_test = multioutput.MultiClassEncoder(
                            model=tree.HoeffdingTreeClassifier(
                                grace_period=config["grace_period"],
                                delta=config["delta"],
                                tau=config["tau"],
                            )
                        )
                        candidate_accuracy = first_exp_HPO(
                            model_test, full_stream, v_data, cluster_label_signature
                        )
                        if best_config[0] < candidate_accuracy:
                            best_config = [
                                candidate_accuracy,
                                config,
                                multioutput.MultiClassEncoder(
                                    model=tree.HoeffdingTreeClassifier(
                                        grace_period=config["grace_period"],
                                        delta=config["delta"],
                                        tau=config["tau"],
                                    )
                                ),
                            ]
                    elif m == "CC_HT":
                        model_test = multioutput.ClassifierChain(
                            model=tree.HoeffdingTreeClassifier(
                                grace_period=config["grace_period"],
                                delta=config["delta"],
                                tau=config["tau"],
                            )
                        )
                        candidate_accuracy = first_exp_HPO(
                            model_test, full_stream, v_data, cluster_label_signature
                        )
                        if best_config[0] < candidate_accuracy:
                            best_config = [
                                candidate_accuracy,
                                config,
                                multioutput.ClassifierChain(
                                    model=tree.HoeffdingTreeClassifier(
                                        grace_period=config["grace_period"],
                                        delta=config["delta"],
                                        tau=config["tau"],
                                    )
                                ),
                            ]
                    elif m == "BR_random_forest":
                        model_test = (
                            implemented_models.binary_relevance.binary_relevance(
                                forest.ARFClassifier(
                                    n_models=config["n_models"],
                                    grace_period=config["grace_period"],
                                    delta=config["delta"],
                                    tau=config["tau"],
                                )
                            )
                        )
                        candidate_accuracy = first_exp_HPO(
                            model_test, full_stream, v_data, cluster_label_signature
                        )
                        if best_config[0] < candidate_accuracy:
                            best_config = [
                                candidate_accuracy,
                                config,
                                implemented_models.binary_relevance.binary_relevance(
                                    forest.ARFClassifier(
                                        n_models=config["n_models"],
                                        grace_period=config["grace_period"],
                                        delta=config["delta"],
                                        tau=config["tau"],
                                    )
                                ),
                            ]
                    elif m == "iSOUPtree":
                        model_test = tree.iSOUPTreeRegressor(
                            grace_period=config["grace_period"],
                            delta=config["delta"],
                            tau=config["tau"],
                        )
                        candidate_accuracy = first_exp_HPO(
                            model_test, full_stream, v_data, cluster_label_signature
                        )
                        if best_config[0] < candidate_accuracy:
                            best_config = [
                                candidate_accuracy,
                                config,
                                tree.iSOUPTreeRegressor(
                                    grace_period=config["grace_period"],
                                    delta=config["delta"],
                                    tau=config["tau"],
                                ),
                            ]
                    elif m == "baseline_1_NN":
                        model_test = implemented_models.baseline_1_NN.baseline_1_NN(
                            equal_null_cos=config["equal_null_cos"],
                            eucli=config["eucli"],
                            max_window_size=config["max_window_size"],
                            label_size=v_data[0],
                            feature_size=v_data[1],
                        )
                        candidate_accuracy = first_exp_HPO(
                            model_test, full_stream, v_data, cluster_label_signature
                        )
                        if best_config[0] < candidate_accuracy:
                            best_config = [
                                candidate_accuracy,
                                config,
                                implemented_models.baseline_1_NN.baseline_1_NN(
                                    equal_null_cos=config["equal_null_cos"],
                                    eucli=config["eucli"],
                                    max_window_size=config["max_window_size"],
                                    label_size=v_data[0],
                                    feature_size=v_data[1],
                                ),
                            ]
                g += 1

        else:
            for config in list_config:
                if m == "NN":
                    model_test = implemented_models.NN.NN_classifier(
                        learning_rate=config["learning_rate"],
                        feature_size=v_data[1],
                        label_size=v_data[0],
                    )
                    candidate_accuracy = first_exp_HPO(
                        model_test, full_stream, v_data, cluster_label_signature
                    )
                    if best_config[0] < candidate_accuracy:
                        best_config = [
                            candidate_accuracy,
                            config,
                            implemented_models.NN.NN_classifier(
                                learning_rate=config["learning_rate"],
                                feature_size=v_data[1],
                                label_size=v_data[0],
                            ),
                        ]
                elif m == "NN_TL":
                    model_test = implemented_models.NN_TL.NN_TL_classifier(
                        learning_rate=config["learning_rate"],
                        feature_size=v_data[1],
                        label_size=v_data[0],
                    )
                    candidate_accuracy = first_exp_HPO(
                        model_test, full_stream, v_data, cluster_label_signature
                    )
                    if best_config[0] < candidate_accuracy:
                        best_config = [
                            candidate_accuracy,
                            config,
                            implemented_models.NN_TL.NN_TL_classifier(
                                learning_rate=config["learning_rate"],
                                feature_size=v_data[1],
                                label_size=v_data[0],
                            ),
                        ]
                elif m == "NN_TLH":
                    model_test = implemented_models.NN_TLH.NN_TLH_classifier(
                        learning_rate=config["learning_rate"],
                        feature_size=v_data[1],
                        hidden_sizes=config["hidden_sizes"],
                        label_size=v_data[0],
                    )
                    candidate_accuracy = first_exp_HPO(
                        model_test, full_stream, v_data, cluster_label_signature
                    )
                    if best_config[0] < candidate_accuracy:
                        best_config = [
                            candidate_accuracy,
                            config,
                            implemented_models.NN_TLH.NN_TLH_classifier(
                                learning_rate=config["learning_rate"],
                                feature_size=v_data[1],
                                hidden_sizes=config["hidden_sizes"],
                                label_size=v_data[0],
                            ),
                        ]
                elif m == "NN_TLH_fifo":
                    model_test = implemented_models.NN_TLH_fifo.NN_TLH_fifo_classifier(
                        learning_rate=config["learning_rate"],
                        feature_size=v_data[1],
                        hidden_sizes=config["hidden_sizes"],
                        label_size=v_data[0],
                        size_replay_fifo=config["size_replay_fifo"],
                        replay_fifo=config["replay_fifo"],
                    )
                    candidate_accuracy = first_exp_HPO(
                        model_test, full_stream, v_data, cluster_label_signature
                    )
                    if best_config[0] < candidate_accuracy:
                        best_config = [
                            candidate_accuracy,
                            config,
                            implemented_models.NN_TLH_fifo.NN_TLH_fifo_classifier(
                                learning_rate=config["learning_rate"],
                                feature_size=v_data[1],
                                hidden_sizes=config["hidden_sizes"],
                                label_size=v_data[0],
                                size_replay_fifo=config["size_replay_fifo"],
                                replay_fifo=config["replay_fifo"],
                            ),
                        ]
                elif m == "NN_TLH_sampling":
                    model_test = (
                        implemented_models.NN_TLH_sampling.NN_TLH_sampling_classifier(
                            learning_rate=config["learning_rate"],
                            feature_size=v_data[1],
                            hidden_sizes=config["hidden_sizes"],
                            label_size=v_data[0],
                            size_replay_sampling=config["size_replay_sampling"],
                            replay_sampling=config["replay_sampling"],
                        )
                    )
                    candidate_accuracy = first_exp_HPO(
                        model_test, full_stream, v_data, cluster_label_signature
                    )
                    if best_config[0] < candidate_accuracy:
                        best_config = [
                            candidate_accuracy,
                            config,
                            implemented_models.NN_TLH_sampling.NN_TLH_sampling_classifier(
                                learning_rate=config["learning_rate"],
                                feature_size=v_data[1],
                                hidden_sizes=config["hidden_sizes"],
                                label_size=v_data[0],
                                size_replay_sampling=config["size_replay_sampling"],
                                replay_sampling=config["replay_sampling"],
                            ),
                        ]
                elif m == "NN_TLH_memories":
                    model_test = (
                        implemented_models.NN_TLH_memories.NN_TLH_memories_classifier(
                            learning_rate=config["learning_rate"],
                            feature_size=v_data[1],
                            hidden_sizes=config["hidden_sizes"],
                            label_size=v_data[0],
                            size_replay_fifo=config["size_replay_fifo"],
                            replay_fifo=config["replay_fifo"],
                            size_replay_sampling=config["size_replay_sampling"],
                            replay_sampling=config["replay_sampling"],
                        )
                    )
                    candidate_accuracy = first_exp_HPO(
                        model_test, full_stream, v_data, cluster_label_signature
                    )
                    if best_config[0] < candidate_accuracy:
                        best_config = [
                            candidate_accuracy,
                            config,
                            implemented_models.NN_TLH_memories.NN_TLH_memories_classifier(
                                learning_rate=config["learning_rate"],
                                feature_size=v_data[1],
                                hidden_sizes=config["hidden_sizes"],
                                label_size=v_data[0],
                                size_replay_fifo=config["size_replay_fifo"],
                                replay_fifo=config["replay_fifo"],
                                size_replay_sampling=config["size_replay_sampling"],
                                replay_sampling=config["replay_sampling"],
                            ),
                        ]
                elif m == "NN_TLH_mini_memories":
                    model_test = implemented_models.NN_TLH_mini_memories.NN_TLH_mini_memories_classifier(
                        learning_rate=config["learning_rate"],
                        feature_size=v_data[1],
                        hidden_sizes=config["hidden_sizes"],
                        label_size=v_data[0],
                        size_replay_fifo=config["size_replay_fifo"],
                        replay_fifo=config["replay_fifo"],
                        size_replay_sampling=config["size_replay_sampling"],
                        replay_sampling=config["replay_sampling"],
                    )
                    candidate_accuracy = first_exp_HPO(
                        model_test, full_stream, v_data, cluster_label_signature
                    )
                    if best_config[0] < candidate_accuracy:
                        best_config = [
                            candidate_accuracy,
                            config,
                            implemented_models.NN_TLH_mini_memories.NN_TLH_mini_memories_classifier(
                                learning_rate=config["learning_rate"],
                                feature_size=v_data[1],
                                hidden_sizes=config["hidden_sizes"],
                                label_size=v_data[0],
                                size_replay_fifo=config["size_replay_fifo"],
                                replay_fifo=config["replay_fifo"],
                                size_replay_sampling=config["size_replay_sampling"],
                                replay_sampling=config["replay_sampling"],
                            ),
                        ]
                elif m == "NN_TLH_attention":
                        # e.g. for each combination of hyperparams:
                        model_test = implemented_models.NN_TLH_attention.NN_TLH_attention_classifier(
                            learning_rate=config["learning_rate"],
                            feature_size=v_data[1],
                            hidden_sizes=config["hidden_sizes"],
                            label_size=v_data[0],
                            size_replay=config["size_replay"],
                            replay_k=config["replay_k"],
                        )

                        candidate_accuracy = first_exp_HPO(
                            model_test, full_stream, v_data, cluster_label_signature
                        )

                        if best_config[0] < candidate_accuracy:
                            best_config = [
                                candidate_accuracy,
                                config,
                                implemented_models.NN_TLH_attention.NN_TLH_attention_classifier(
                                    learning_rate=config["learning_rate"],
                                    feature_size=v_data[1],
                                    hidden_sizes=config["hidden_sizes"],
                                    label_size=v_data[0],
                                    size_replay=config["size_replay"],
                                    replay_k=config["replay_k"],
                                ),
                            ]
                elif m == "NN_HybridAdaptive":
                    model_test = implemented_models.NN_HybridAdaptive.NN_HybridAdaptive_classifier(
                        learning_rate=config["learning_rate"],
                        feature_size=v_data[1],
                        label_size=v_data[0],
                        hidden1=config["hidden1"],
                        hidden2=config["hidden2"],
                        size_fifo=config["size_fifo"],
                        size_reservoir=config["size_reservoir"],
                        replay_samples=config["replay_samples"],
                    )

                    candidate_accuracy = first_exp_HPO(model_test, full_stream, v_data, cluster_label_signature)

                    if best_config[0] < candidate_accuracy:
                        best_config = [candidate_accuracy, config, model_test]
                elif m == "BR_HT":
                    model_test = implemented_models.binary_relevance.binary_relevance(
                        model=tree.HoeffdingTreeClassifier(
                            grace_period=config["grace_period"],
                            delta=config["delta"],
                            tau=config["tau"],
                        )
                    )
                    candidate_accuracy = first_exp_HPO(
                        model_test, full_stream, v_data, cluster_label_signature
                    )
                    if best_config[0] < candidate_accuracy:
                        best_config = [
                            candidate_accuracy,
                            config,
                            implemented_models.binary_relevance.binary_relevance(
                                model=tree.HoeffdingTreeClassifier(
                                    grace_period=config["grace_period"],
                                    delta=config["delta"],
                                    tau=config["tau"],
                                )
                            ),
                        ]
                elif m == "LC_HT":
                    model_test = multioutput.MultiClassEncoder(
                        model=tree.HoeffdingTreeClassifier(
                            grace_period=config["grace_period"],
                            delta=config["delta"],
                            tau=config["tau"],
                        )
                    )
                    candidate_accuracy = first_exp_HPO(
                        model_test, full_stream, v_data, cluster_label_signature
                    )
                    if best_config[0] < candidate_accuracy:
                        best_config = [
                            candidate_accuracy,
                            config,
                            multioutput.MultiClassEncoder(
                                model=tree.HoeffdingTreeClassifier(
                                    grace_period=config["grace_period"],
                                    delta=config["delta"],
                                    tau=config["tau"],
                                )
                            ),
                        ]
                elif m == "CC_HT":
                    model_test = multioutput.ClassifierChain(
                        model=tree.HoeffdingTreeClassifier(
                            grace_period=config["grace_period"],
                            delta=config["delta"],
                            tau=config["tau"],
                        )
                    )
                    candidate_accuracy = first_exp_HPO(
                        model_test, full_stream, v_data, cluster_label_signature
                    )
                    if best_config[0] < candidate_accuracy:
                        best_config = [
                            candidate_accuracy,
                            config,
                            multioutput.ClassifierChain(
                                model=tree.HoeffdingTreeClassifier(
                                    grace_period=config["grace_period"],
                                    delta=config["delta"],
                                    tau=config["tau"],
                                )
                            ),
                        ]
                elif m == "BR_random_forest":
                    model_test = implemented_models.binary_relevance.binary_relevance(
                        forest.ARFClassifier(
                            n_models=config["n_models"],
                            grace_period=config["grace_period"],
                            delta=config["delta"],
                            tau=config["tau"],
                        )
                    )
                    candidate_accuracy = first_exp_HPO(
                        model_test, full_stream, v_data, cluster_label_signature
                    )
                    if best_config[0] < candidate_accuracy:
                        best_config = [
                            candidate_accuracy,
                            config,
                            implemented_models.binary_relevance.binary_relevance(
                                forest.ARFClassifier(
                                    n_models=config["n_models"],
                                    grace_period=config["grace_period"],
                                    delta=config["delta"],
                                    tau=config["tau"],
                                )
                            ),
                        ]
                elif m == "iSOUPtree":
                    model_test = tree.iSOUPTreeRegressor(
                        grace_period=config["grace_period"],
                        delta=config["delta"],
                        tau=config["tau"],
                    )
                    candidate_accuracy = first_exp_HPO(
                        model_test, full_stream, v_data, cluster_label_signature
                    )
                    if best_config[0] < candidate_accuracy:
                        best_config = [
                            candidate_accuracy,
                            config,
                            tree.iSOUPTreeRegressor(
                                grace_period=config["grace_period"],
                                delta=config["delta"],
                                tau=config["tau"],
                            ),
                        ]
                elif m == "baseline_last_class":
                    best_config = [
                        0,
                        0,
                        implemented_models.binary_relevance.binary_relevance(
                            dummy.NoChangeClassifier()
                        ),
                    ]
                elif m == "baseline_prior_distribution":
                    best_config = [
                        0,
                        0,
                        implemented_models.binary_relevance.binary_relevance(
                            dummy.PriorClassifier()
                        ),
                    ]
                elif m == "baseline_mean":
                    best_config = [
                        0,
                        0,
                        implemented_models.binary_relevance.binary_relevance(
                            dummy.StatisticRegressor(stats.Mean())
                        ),
                    ]
                elif m == "baseline_1_NN":
                    model_test = implemented_models.baseline_1_NN.baseline_1_NN(
                        equal_null_cos=config["equal_null_cos"],
                        eucli=["eucli"],
                        max_window_size=["max_window_size"],
                        label_size=v_data[0],
                        feature_size=v_data[1],
                    )
                    candidate_accuracy = first_exp_HPO(
                        model_test, full_stream, v_data, cluster_label_signature
                    )
                    if best_config[0] < candidate_accuracy:
                        best_config = [
                            candidate_accuracy,
                            config,
                            implemented_models.baseline_1_NN.baseline_1_NN(
                                equal_null_cos=config["equal_null_cos"],
                                eucli=config["eucli"],
                                max_window_size=["max_window_size"],
                                label_size=v_data[0],
                                feature_size=v_data[1],
                            ),
                        ]

        with open(
            workdir
            + "Config/{}".format(k_data)
            + "_{}_".format(m)
            + "{}.json".format(scenario),
            "w",
        ) as outfile:
            json.dump(best_config[1], outfile)

        # Initializing the metrics
        min_task_size = 100000
        for k, v in full_stream.items():
            min_task_size = min(
                len(cluster_label_signature["Cluster {}".format(v[0])]),
                min_task_size,
            )
        nb_top = min(3, min_task_size)

        online_metrics_dict = dict()
        metrics_online_results = dict()

        online_metrics_dict = {
            "precisionatk": bench_metrics.precisionatk.precisionatk(nb_top),
            "RMSE": bench_metrics.RMSE.RMSE(),
            "macro_BA": metrics.multioutput.MacroAverage(metrics.BalancedAccuracy()),
        }
        metrics_online_results = {
            "precisionatk": [],
            "RMSE": [],
            "macro_BA": [],
        }

        metrics_continual_results = dict()
        continual_metrics_dict = dict()
        iter_continual = 0

        # Seen clusters and labels memory
        seen_clusters = []
        seen_labels = []

        # Initializing the code carbon tracker
        tracker = OfflineEmissionsTracker(
            tracking_mode="process",
            output_dir="consumption/",
            output_file="{}_".format(k_data)
            + "{}_".format(m)
            + "{}_".format(ordering)
            + "{}_".format(scenario)
            + "consumption"
            + ".csv",
            country_iso_code="FRA",
        )
        tracker.start()
        time_start = time.time()

        model_final = []

        if m != "baseline_oracle":
            model_final = best_config[2]

        # Benchmark
        for k_stream, v_stream in full_stream.items():
            if time.time() - time_start > 28800:
                break
            if v_stream[0] not in seen_clusters:
                # NEW LINES: Extend seen_labels with the newly encountered cluster’s labels
                seen_labels.extend(cluster_label_signature[f"Cluster {v_stream[0]}"])
                # Drop duplicates if needed
                seen_labels = list(set(seen_labels))

                seen_clusters.append(v_stream[0])
            X_frame = v_stream[1].iloc[:, : (-1) * (v_data[0])]
            Y_frame = v_stream[1].iloc[:, (-1) * (v_data[0]) :]
            for x, y in stream.iter_pandas(X_frame, Y_frame):
                if time.time() - time_start > 28800:
                    break
                # Test-then-train protocole :
                # Prediction :
                if m != "baseline_oracle":
                    y_pred = model_final.predict_one(x)
                elif m == "baseline_oracle":
                    y_pred = y
                # Online evaluation :
                online_eval(
                    online_metrics_dict,
                    y,
                    y_pred,
                    metrics_online_results,
                    v_stream[0],
                )
                # Training
                if (
                    m == "NN_TL"
                    or m == "NN_TLH"
                    or m == "NN_TLH_fifo"
                    or m == "NN_TLH_sampling"
                    or m == "NN_TLH_memories"
                    or m == "NN_TLH_mini_memories"
                    or m == "NN_TLH_attention"
                    or m == "NN_HybridAdaptive"
                ):
                    model_final.learn_one(
                        x,
                        y,
                        cluster_label_signature["Cluster {}".format(v_stream[0])],
                    )
                else:
                    if m != "baseline_oracle":
                        model_final.learn_one(x, y)

            # Initializing the metrics for continual evaluation :
            init_continual_metrics(
                eval_sets,
                continual_metrics_dict,
                metrics_continual_results,
                iter_continual,
            )

            # Continual evaluation :
            for k_eval, v_eval in eval_sets.items():
                if time.time() - time_start > 28800:
                    break
                for x, y in stream.iter_pandas(
                    eval_sets[k_eval][0], eval_sets[k_eval][1]
                ):
                    if time.time() - time_start > 28800:
                        break
                    # Prediction
                    if m != "baseline_oracle":
                        y_pred = model_final.predict_one(x)
                    elif m == "baseline_oracle":
                        y_pred = y
                    # Evaluation
                    continual_eval(
                        continual_metrics_dict, k_eval, iter_continual, y, y_pred
                    )
                # Saving continual metrics results in a dict
                continual_results_saving(
                    metrics_continual_results,
                    continual_metrics_dict,
                    k_eval,
                    iter_continual,
                )
            iter_continual += 1
        # Stop code carbon tracker
        emissions: float = tracker.stop()

        # Saving results files
        save_bench_results(
            k_data,
            m,
            ordering,
            metrics_online_results,
            metrics_continual_results,
            workdir,
        )
